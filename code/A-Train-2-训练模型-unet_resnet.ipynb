{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonard/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from utils.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = PATH['model_train']\n",
    "src_val = PATH['model_val']\n",
    "model_paths = PATH['model_paths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(x,nb_filter,kernel_size=3):\n",
    "    k1,k2,k3 = nb_filter\n",
    "    out = Convolution2D(k1,(1,1))(x)\n",
    "    out = BatchNormalization(axis=-1)(out)\n",
    "    out = Activation('relu')(out)\n",
    "\n",
    "    out = Convolution2D(k2,(kernel_size,kernel_size),border_mode='same')(out)\n",
    "    out = BatchNormalization(axis=-1)(out)\n",
    "    out = Activation('relu')(out)\n",
    "\n",
    "    out = Convolution2D(k3,(1,1))(out)\n",
    "    out = BatchNormalization(axis=-1)(out)\n",
    "\n",
    "    out = merge([out,x],mode='sum')\n",
    "    out = Activation('relu')(out)\n",
    "    return out\n",
    "\n",
    "def conv_block(x, nb_filter, kernel_size=3, strides=(2, 2)):\n",
    "    k1, k2, k3 = nb_filter\n",
    "\n",
    "    out = Convolution2D(k1, (1, 1), strides=strides)(x)\n",
    "    out = BatchNormalization(axis=-1)(out)\n",
    "    out = Activation('relu')(out)\n",
    "\n",
    "    out = Convolution2D(k2,(kernel_size,kernel_size),border_mode='same')(out)\n",
    "    out = BatchNormalization(axis=-1)(out)\n",
    "    out = Activation('relu')(out)\n",
    "\n",
    "    out = Convolution2D(k3, (1, 1))(out)\n",
    "    out = BatchNormalization(axis=-1)(out)\n",
    "\n",
    "    shortcut = Convolution2D(k3, (1, 1), strides=strides)(x)\n",
    "    shortcut = BatchNormalization(axis=-1)(shortcut)\n",
    "\n",
    "    out = merge([out, shortcut],mode='sum')\n",
    "    out = Activation('relu')(out)\n",
    "    return out\n",
    "\n",
    "def unet_model(dropout_rate,learn_rate,width):\n",
    "    inputs = Input((1, 512,512))       \n",
    "    # Normalization\n",
    "    #x = Lambda(lambda x: x / 255, name='pre-process')(inputs)\n",
    "    x = Convolution2D(width, (3, 3), strides=(1, 1), padding='same',activation=\"elu\")(inputs)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    # Block 1\n",
    "    c1 = conv_block(x,(width/2, width/2,width),strides=(1,1))\n",
    "    c1 = identity_block(c1,(width/2, width/2,width))      # 128,512,512\n",
    "   \n",
    "    #p1 = MaxPooling2D(pool_size=(2, 2))(c1)\n",
    "    \n",
    "    # Block 2\n",
    "    c2 = Convolution2D(width*2, (3, 3), padding=\"same\", activation=\"elu\")(c1)\n",
    "    c2 = conv_block(c2,(width,width,width*2),strides=(2,2))\n",
    "    c2 = identity_block(c2,(width,width,width*2))    # 256,256,256\n",
    "    #p2 = MaxPooling2D(pool_size=(2, 2))(c2)\n",
    "    \n",
    "    # Block 3\n",
    "    c3 = Convolution2D(width*4, (3, 3), padding=\"same\", activation=\"elu\")(c2)\n",
    "    c3 = conv_block(c3,(width*2, width*2,width*4),strides=[2,2])\n",
    "    c3 = identity_block(c3,(width*2, width*2,width*4))   # 80, 120, 3\n",
    "    #p3 = MaxPooling2D(pool_size=(2, 2))(c3)\n",
    "    print(c3)\n",
    "    # Block 4\n",
    "    c4 = Convolution2D(width*8, (3, 3), padding=\"same\", activation=\"elu\")(c3)\n",
    "    c4 = conv_block(c4,(width*4, width*4,width*8),strides=[2,2])\n",
    "    c4 = identity_block(c4,(width*4, width*4,width*8))   # 40, 60, 3\n",
    "    #p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "    \n",
    "    # Block 5\n",
    "    c5 = Convolution2D(width*16, (3, 3), padding=\"same\", activation=\"elu\")(c4)\n",
    "    c5 = conv_block(c5,(width*8, width*8,width*16),strides=[2,2])\n",
    "    c5 = identity_block(c5,(width*8, width*8,width*16)) # 20, 30, 3\n",
    "    \n",
    "    # Block 6\n",
    "    u6 = merge([UpSampling2D(size=(2, 2))(c5), c4], mode='concat', concat_axis=1)\n",
    "    c6 = SpatialDropout2D(dropout_rate)(u6)\n",
    "    c6 = Convolution2D(width*8, (3, 3), padding=\"same\", activation=\"elu\")(c6)\n",
    "    c6 = conv_block(c6,(width*4, width*4,width*8),strides=[1,1])\n",
    "    c6 = identity_block(c6,(width*4, width*4,width*8))\n",
    "\n",
    "    # Block 7\n",
    "    u7 = merge([UpSampling2D(size=(2, 2))(c6), c3], mode='concat', concat_axis=1)\n",
    "    c7 = SpatialDropout2D(dropout_rate)(u7)\n",
    "    c7 = Convolution2D(width*4, (3, 3), padding=\"same\", activation=\"elu\")(c7)\n",
    "    c7 = conv_block(c7,(width*2, width*2,width*4),strides=[1,1])\n",
    "    c7 = identity_block(c7,(width*2, width*2,width*4))\n",
    "\n",
    "    # Block 8\n",
    "    u8 = merge([UpSampling2D(size=(2, 2))(c7), c2], mode='concat', concat_axis=1)\n",
    "    c8 = SpatialDropout2D(dropout_rate)(u8)\n",
    "    c8 = Convolution2D(width*2, (3, 3), padding=\"same\", activation=\"elu\")(c8)\n",
    "    c8 = conv_block(c8,(width, width,width*2),strides=[1,1])\n",
    "    c8 = identity_block(c8, (width, width,width*2))\n",
    "\n",
    "    # Block 9\n",
    "    u9 = merge([UpSampling2D(size=(2, 2))(c8), c1], mode='concat', concat_axis=1)\n",
    "    c9 = SpatialDropout2D(dropout_rate)(u9)\n",
    "    c9 = Convolution2D(width, (3, 3), padding=\"same\", activation=\"elu\")(c9)\n",
    "    c9 = conv_block(c9,(width/2, width/2,width),strides=[1,1])\n",
    "    c9 = identity_block(c9,(width/2, width/2,width))\n",
    "    c10 = Convolution2D(1, (1, 1), activation=\"sigmoid\")(c9)\n",
    "\n",
    "    model = Model(input=inputs, output=c10)\n",
    "    #model.summary()\n",
    "    model.compile(optimizer=Adam(lr=learn_rate), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    #model.compile(optimizer=SGD(lr=learn_rate, momentum=0.9, nesterov=True), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    \n",
    "    #plot_model(model, to_file='model1.png',show_shapes=True)\n",
    "    return model\n",
    "\n",
    "def unet_fit(name, check_name = None):\n",
    "    data_gen_args = dict(rotation_range=30., \n",
    "                         width_shift_range=0.3,  \n",
    "                         height_shift_range=0.3,   \n",
    "                         horizontal_flip=True,   \n",
    "                         vertical_flip=True, \n",
    "                     \n",
    "                     )\n",
    "    from keras.preprocessing.image import ImageDataGenerator\n",
    "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    \n",
    "    # Provide the same seed and keyword arguments to the fit and flow methods\n",
    "    seed = 1\n",
    "    \n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        src,\n",
    "        class_mode=None,\n",
    "        classes=['lung'],\n",
    "        seed=seed,\n",
    "        target_size=(512,512),\n",
    "        color_mode=\"grayscale\",\n",
    "        batch_size=1)\n",
    "\n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        src,\n",
    "        class_mode=None,\n",
    "        classes=['nodule'],\n",
    "        seed=seed,\n",
    "        target_size=(512,512),\n",
    "        color_mode=\"grayscale\",\n",
    "        batch_size=1) \n",
    "    datagen_val = ImageDataGenerator()\n",
    "    image_generator_val = datagen_val.flow_from_directory(\n",
    "        src_val,\n",
    "        class_mode=None,\n",
    "        classes=['lung'],\n",
    "        seed=seed,\n",
    "        target_size=(512,512),\n",
    "        color_mode=\"grayscale\",\n",
    "        batch_size=1)\n",
    "\n",
    "    mask_generator_val = datagen_val.flow_from_directory(\n",
    "        src_val,\n",
    "        class_mode=None,\n",
    "        classes=['nodule'],\n",
    "        seed=seed,\n",
    "        target_size=(512,512),\n",
    "        color_mode=\"grayscale\",\n",
    "        batch_size=1) \n",
    "    # combine generators into one which yields image and masks\n",
    "    train_generator = itertools.izip(image_generator, mask_generator) \n",
    "    val_generator = itertools.izip(image_generator_val, mask_generator_val)\n",
    "    t = time.time()\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience = 20, \n",
    "                                   verbose = 1),\n",
    "    ModelCheckpoint(model_paths + '{}.h5'.format(name), \n",
    "                        monitor='val_loss', \n",
    "                        verbose = 0, save_best_only = True)]\n",
    "    \n",
    "    if check_name is not None:\n",
    "        check_model = model_paths + '{}.h5'.format(check_name)\n",
    "        model = load_model(check_model, \n",
    "                           custom_objects={'dice_coef_loss': dice_coef_loss, 'dice_coef': dice_coef})\n",
    "    else:\n",
    "        model = unet_model(dropout_rate = 0.30, learn_rate = 1e-5,width=64)\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        epochs=300,\n",
    "        verbose =1, \n",
    "        callbacks = callbacks,\n",
    "        steps_per_epoch=1280,\n",
    "        validation_data = val_generator,\n",
    "        nb_val_samples = 256)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2913 images belonging to 1 classes.\n",
      "Found 2913 images belonging to 1 classes.\n",
      "Found 798 images belonging to 1 classes.\n",
      "Found 798 images belonging to 1 classes.\n",
      "Tensor(\"activation_18/Relu:0\", shape=(?, 256, 128, 128), dtype=float32)\n",
      "Epoch 1/300\n",
      "1280/1280 [==============================] - 563s 440ms/step - loss: 0.6813 - dice_coef: 0.3187 - val_loss: 0.4997 - val_dice_coef: 0.5003\n",
      "Epoch 2/300\n",
      "1280/1280 [==============================] - 557s 435ms/step - loss: 0.5018 - dice_coef: 0.4982 - val_loss: 0.4563 - val_dice_coef: 0.5437\n",
      "Epoch 3/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.4699 - dice_coef: 0.5301 - val_loss: 0.4847 - val_dice_coef: 0.5153\n",
      "Epoch 4/300\n",
      "1280/1280 [==============================] - 559s 436ms/step - loss: 0.4149 - dice_coef: 0.5851 - val_loss: 0.3631 - val_dice_coef: 0.6369\n",
      "Epoch 5/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.4046 - dice_coef: 0.5954 - val_loss: 0.4172 - val_dice_coef: 0.5828\n",
      "Epoch 6/300\n",
      "1280/1280 [==============================] - 559s 437ms/step - loss: 0.4006 - dice_coef: 0.5994 - val_loss: 0.3145 - val_dice_coef: 0.6855\n",
      "Epoch 7/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.3618 - dice_coef: 0.6382 - val_loss: 0.3314 - val_dice_coef: 0.6686\n",
      "Epoch 8/300\n",
      "1280/1280 [==============================] - 559s 437ms/step - loss: 0.3696 - dice_coef: 0.6304 - val_loss: 0.3319 - val_dice_coef: 0.6681\n",
      "Epoch 9/300\n",
      "1280/1280 [==============================] - 559s 437ms/step - loss: 0.3325 - dice_coef: 0.6675 - val_loss: 0.3273 - val_dice_coef: 0.6727\n",
      "Epoch 10/300\n",
      "1280/1280 [==============================] - 559s 437ms/step - loss: 0.3296 - dice_coef: 0.6704 - val_loss: 0.3184 - val_dice_coef: 0.6816\n",
      "Epoch 11/300\n",
      "1280/1280 [==============================] - 559s 437ms/step - loss: 0.3102 - dice_coef: 0.6898 - val_loss: 0.2668 - val_dice_coef: 0.7332\n",
      "Epoch 12/300\n",
      "1280/1280 [==============================] - 559s 437ms/step - loss: 0.3003 - dice_coef: 0.6997 - val_loss: 0.2488 - val_dice_coef: 0.7512\n",
      "Epoch 13/300\n",
      "1280/1280 [==============================] - 559s 437ms/step - loss: 0.3092 - dice_coef: 0.6908 - val_loss: 0.2701 - val_dice_coef: 0.7299\n",
      "Epoch 14/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2936 - dice_coef: 0.7064 - val_loss: 0.3377 - val_dice_coef: 0.6623\n",
      "Epoch 15/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2776 - dice_coef: 0.7224 - val_loss: 0.1858 - val_dice_coef: 0.8142\n",
      "Epoch 16/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2930 - dice_coef: 0.7070 - val_loss: 0.2227 - val_dice_coef: 0.7773\n",
      "Epoch 17/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2782 - dice_coef: 0.7218 - val_loss: 0.2908 - val_dice_coef: 0.7092\n",
      "Epoch 18/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2845 - dice_coef: 0.7155 - val_loss: 0.2407 - val_dice_coef: 0.7593\n",
      "Epoch 19/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2650 - dice_coef: 0.7350 - val_loss: 0.2769 - val_dice_coef: 0.7231\n",
      "Epoch 20/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2913 - dice_coef: 0.7087 - val_loss: 0.2409 - val_dice_coef: 0.7591\n",
      "Epoch 21/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2604 - dice_coef: 0.7396 - val_loss: 0.2060 - val_dice_coef: 0.7940\n",
      "Epoch 22/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2756 - dice_coef: 0.7244 - val_loss: 0.2941 - val_dice_coef: 0.7059\n",
      "Epoch 23/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2527 - dice_coef: 0.7473 - val_loss: 0.2053 - val_dice_coef: 0.7947\n",
      "Epoch 24/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2648 - dice_coef: 0.7352 - val_loss: 0.1872 - val_dice_coef: 0.8128\n",
      "Epoch 25/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2474 - dice_coef: 0.7526 - val_loss: 0.1784 - val_dice_coef: 0.8216\n",
      "Epoch 26/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2558 - dice_coef: 0.7442 - val_loss: 0.2544 - val_dice_coef: 0.7456\n",
      "Epoch 27/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2554 - dice_coef: 0.7446 - val_loss: 0.1748 - val_dice_coef: 0.8252\n",
      "Epoch 28/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2450 - dice_coef: 0.7550 - val_loss: 0.2251 - val_dice_coef: 0.7749\n",
      "Epoch 29/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2454 - dice_coef: 0.7546 - val_loss: 0.1875 - val_dice_coef: 0.8125\n",
      "Epoch 30/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2491 - dice_coef: 0.7509 - val_loss: 0.1972 - val_dice_coef: 0.8028\n",
      "Epoch 31/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2450 - dice_coef: 0.7550 - val_loss: 0.1802 - val_dice_coef: 0.8198\n",
      "Epoch 32/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2544 - dice_coef: 0.7456 - val_loss: 0.2982 - val_dice_coef: 0.7018\n",
      "Epoch 33/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2356 - dice_coef: 0.7644 - val_loss: 0.1909 - val_dice_coef: 0.8091\n",
      "Epoch 34/300\n",
      "1280/1280 [==============================] - 557s 435ms/step - loss: 0.2659 - dice_coef: 0.7341 - val_loss: 0.1785 - val_dice_coef: 0.8215\n",
      "Epoch 35/300\n",
      "1280/1280 [==============================] - 557s 436ms/step - loss: 0.2559 - dice_coef: 0.7441 - val_loss: 0.1924 - val_dice_coef: 0.8076\n",
      "Epoch 36/300\n",
      "1280/1280 [==============================] - 557s 436ms/step - loss: 0.2355 - dice_coef: 0.7645 - val_loss: 0.1756 - val_dice_coef: 0.8244\n",
      "Epoch 37/300\n",
      "1280/1280 [==============================] - 557s 436ms/step - loss: 0.2615 - dice_coef: 0.7385 - val_loss: 0.2331 - val_dice_coef: 0.7669\n",
      "Epoch 38/300\n",
      "1280/1280 [==============================] - 557s 435ms/step - loss: 0.2393 - dice_coef: 0.7607 - val_loss: 0.1894 - val_dice_coef: 0.8106\n",
      "Epoch 39/300\n",
      "1280/1280 [==============================] - 557s 435ms/step - loss: 0.2305 - dice_coef: 0.7695 - val_loss: 0.3277 - val_dice_coef: 0.6723\n",
      "Epoch 40/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2489 - dice_coef: 0.7511 - val_loss: 0.2100 - val_dice_coef: 0.7900\n",
      "Epoch 41/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2443 - dice_coef: 0.7557 - val_loss: 0.1667 - val_dice_coef: 0.8333\n",
      "Epoch 42/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2377 - dice_coef: 0.7623 - val_loss: 0.2078 - val_dice_coef: 0.7922\n",
      "Epoch 43/300\n",
      "1280/1280 [==============================] - 560s 437ms/step - loss: 0.2305 - dice_coef: 0.7695 - val_loss: 0.2149 - val_dice_coef: 0.7851\n",
      "Epoch 44/300\n",
      "1280/1280 [==============================] - 561s 438ms/step - loss: 0.2440 - dice_coef: 0.7560 - val_loss: 0.1878 - val_dice_coef: 0.8122\n",
      "Epoch 45/300\n",
      "1280/1280 [==============================] - 561s 438ms/step - loss: 0.2457 - dice_coef: 0.7543 - val_loss: 0.1934 - val_dice_coef: 0.8066\n",
      "Epoch 46/300\n",
      "1280/1280 [==============================] - 561s 438ms/step - loss: 0.2309 - dice_coef: 0.7691 - val_loss: 0.2249 - val_dice_coef: 0.7751\n",
      "Epoch 47/300\n",
      "1280/1280 [==============================] - 561s 438ms/step - loss: 0.2318 - dice_coef: 0.7682 - val_loss: 0.2150 - val_dice_coef: 0.7850\n",
      "Epoch 48/300\n",
      "1280/1280 [==============================] - 561s 438ms/step - loss: 0.2163 - dice_coef: 0.7837 - val_loss: 0.2409 - val_dice_coef: 0.7591\n",
      "Epoch 49/300\n",
      "1280/1280 [==============================] - 562s 439ms/step - loss: 0.2396 - dice_coef: 0.7604 - val_loss: 0.1611 - val_dice_coef: 0.8389\n",
      "Epoch 50/300\n",
      "1280/1280 [==============================] - 561s 439ms/step - loss: 0.2218 - dice_coef: 0.7782 - val_loss: 0.1879 - val_dice_coef: 0.8121\n",
      "Epoch 51/300\n",
      "1280/1280 [==============================] - 562s 439ms/step - loss: 0.2457 - dice_coef: 0.7543 - val_loss: 0.1573 - val_dice_coef: 0.8427\n",
      "Epoch 52/300\n",
      "1280/1280 [==============================] - 561s 439ms/step - loss: 0.2366 - dice_coef: 0.7634 - val_loss: 0.2157 - val_dice_coef: 0.7843\n",
      "Epoch 53/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280/1280 [==============================] - 556s 434ms/step - loss: 0.2209 - dice_coef: 0.7791 - val_loss: 0.1669 - val_dice_coef: 0.8331\n",
      "Epoch 54/300\n",
      "1280/1280 [==============================] - 556s 435ms/step - loss: 0.2280 - dice_coef: 0.7720 - val_loss: 0.1322 - val_dice_coef: 0.8678\n",
      "Epoch 55/300\n",
      "1280/1280 [==============================] - 557s 435ms/step - loss: 0.2256 - dice_coef: 0.7744 - val_loss: 0.2176 - val_dice_coef: 0.7824\n",
      "Epoch 56/300\n",
      "1280/1280 [==============================] - 557s 435ms/step - loss: 0.2028 - dice_coef: 0.7972 - val_loss: 0.1966 - val_dice_coef: 0.8034\n",
      "Epoch 57/300\n",
      "1280/1280 [==============================] - 557s 435ms/step - loss: 0.2465 - dice_coef: 0.7535 - val_loss: 0.1675 - val_dice_coef: 0.8325\n",
      "Epoch 58/300\n",
      "1280/1280 [==============================] - 557s 435ms/step - loss: 0.2104 - dice_coef: 0.7896 - val_loss: 0.1958 - val_dice_coef: 0.8042\n",
      "Epoch 59/300\n",
      "1280/1280 [==============================] - 557s 435ms/step - loss: 0.2270 - dice_coef: 0.7730 - val_loss: 0.2040 - val_dice_coef: 0.7960\n",
      "Epoch 60/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2357 - dice_coef: 0.7643 - val_loss: 0.1684 - val_dice_coef: 0.8316\n",
      "Epoch 61/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2221 - dice_coef: 0.7779 - val_loss: 0.1750 - val_dice_coef: 0.8250\n",
      "Epoch 62/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2278 - dice_coef: 0.7722 - val_loss: 0.1583 - val_dice_coef: 0.8417\n",
      "Epoch 63/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2199 - dice_coef: 0.7801 - val_loss: 0.1934 - val_dice_coef: 0.8066\n",
      "Epoch 64/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2229 - dice_coef: 0.7771 - val_loss: 0.1776 - val_dice_coef: 0.8224\n",
      "Epoch 65/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2271 - dice_coef: 0.7729 - val_loss: 0.1673 - val_dice_coef: 0.8327\n",
      "Epoch 66/300\n",
      "1280/1280 [==============================] - 559s 436ms/step - loss: 0.2238 - dice_coef: 0.7762 - val_loss: 0.1719 - val_dice_coef: 0.8281\n",
      "Epoch 67/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2314 - dice_coef: 0.7686 - val_loss: 0.1579 - val_dice_coef: 0.8421\n",
      "Epoch 68/300\n",
      "1280/1280 [==============================] - 559s 436ms/step - loss: 0.2140 - dice_coef: 0.7860 - val_loss: 0.1912 - val_dice_coef: 0.8088\n",
      "Epoch 69/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2285 - dice_coef: 0.7715 - val_loss: 0.1861 - val_dice_coef: 0.8139\n",
      "Epoch 70/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2264 - dice_coef: 0.7736 - val_loss: 0.1519 - val_dice_coef: 0.8481\n",
      "Epoch 71/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2358 - dice_coef: 0.7642 - val_loss: 0.2038 - val_dice_coef: 0.7962\n",
      "Epoch 72/300\n",
      "1280/1280 [==============================] - 558s 436ms/step - loss: 0.2254 - dice_coef: 0.7746 - val_loss: 0.2095 - val_dice_coef: 0.7905\n",
      "Epoch 73/300\n",
      "1280/1280 [==============================] - 559s 436ms/step - loss: 0.2143 - dice_coef: 0.7857 - val_loss: 0.1451 - val_dice_coef: 0.8549\n",
      "Epoch 74/300\n",
      "1280/1280 [==============================] - 559s 436ms/step - loss: 0.2102 - dice_coef: 0.7898 - val_loss: 0.1921 - val_dice_coef: 0.8079\n",
      "Epoch 00074: early stopping\n"
     ]
    }
   ],
   "source": [
    "unet_fit('final_fenge_Resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
